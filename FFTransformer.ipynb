{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A. ENCODER PART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 300\n",
    "n_head = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(x):\n",
    "    return x.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(x):\n",
    "    embeddings = wv[x]\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "def posEncode(x):\n",
    "    n_seq, _ = x.shape\n",
    "    i = np.arange(0, d_model, 2, dtype='float16')\n",
    "    denominator = np.power(10000, i/d_model)\n",
    "    position = np.arange(0, n_seq, dtype='float16').reshape(-1,1)\n",
    "    even_PE = np.sin(position / denominator)\n",
    "    odd_PE  = np.cos(position / denominator)\n",
    "    return x + np.ravel([even_PE.T, odd_PE.T],'F').reshape(n_seq, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masking(x):\n",
    "    mask = np.tril(np.ones((len(x), len(x))))\n",
    "    mask[mask==0] = -np.infty\n",
    "    mask[mask==1] = 0\n",
    "    x = x + mask\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'hi my name is bill I am from Canada'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenize(sentence)\n",
    "embeddings = embed(tokens)\n",
    "input = posEncode(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding Attention /\n",
    "4 heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_QKV():\n",
    "    #used once only\n",
    "    Q, K, V = [],[],[]\n",
    "    for h in range(n_head):\n",
    "        Q.append(np.random.rand(d_model, d_model//n_head)-0.5)\n",
    "        K.append(np.random.rand(d_model, d_model//n_head)-0.5)\n",
    "        V.append(np.random.rand(d_model, d_model//n_head)-0.5)\n",
    "    return Q, K, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context(input, Q,K,V, mask=False):    \n",
    "    context = []\n",
    "    for h in range(n_head):\n",
    "        raw_attention = (input @ Q[h]) @ (input @ K[h]).T\n",
    "        if (mask == True):\n",
    "            raw_attention = masking(raw_attention)\n",
    "        \n",
    "        score = tf.nn.softmax(raw_attention / (d_model)** .5)\n",
    "        context.append(score @ (input @ V[h]))\n",
    "\n",
    "    concat = np.concatenate([c for c in context], axis=1)    \n",
    "    return concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q,K,V = init_QKV()\n",
    "context1 = context(input, Q,K,V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add and Normalization\n",
    "Feed Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma1 = np.random.rand(d_model)\n",
    "beta1 = np.random.rand(d_model) \n",
    "\n",
    "gamma2 = np.random.rand(d_model) \n",
    "beta2 = np.random.rand(d_model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = np.random.rand(d_model, d_model) \n",
    "b1 = np.random.rand(d_model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_norm(context, prev_input, gamma, beta):\n",
    "    context = context + prev_input\n",
    "    mean = context.mean(axis=1).reshape(-1,1)\n",
    "    sigma = context.std(axis=1).reshape(-1,1)\n",
    "    context = (context - mean) / sigma\n",
    "    context = context * gamma + beta\n",
    "    return context\n",
    "\n",
    "def feed_forward(context, w, b):\n",
    "    #Assume no hidden layer\n",
    "    context = context @ w + b\n",
    "    context = tf.nn.relu(context).numpy()\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "context2 = add_norm(context1, input, gamma1, beta1)\n",
    "context3 = feed_forward(context2, w1, b1)\n",
    "context4 = add_norm(context3, context2, gamma2, beta2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B. DECODER PART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence2 = \"</s>\"\n",
    "input2 = posEncode(embed(tokenize(sentence2))) #residual for the next addNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_d, K_d, V_d = init_QKV()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_d_1 = context(input2, Q_d, K_d, V_d, mask=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add and Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_d_1 = np.random.rand(d_model)\n",
    "beta_d_1 = np.random.rand(d_model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_d_2 = add_norm(context_d_1, input2, gamma_d_1, beta_d_1) #residual for the next addNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_context(input_e, input_d, Q_d,K_e,V_e):    \n",
    "    context = []\n",
    "    for h in range(n_head):\n",
    "        raw_attention = (input_d @ Q_d[h]) @ (input_e @ K_e[h]).T\n",
    "        score = tf.nn.softmax(raw_attention / (d_model)** .5)\n",
    "        context.append(score @ (input_e @ V_e[h]))\n",
    "\n",
    "    concat = np.concatenate([c for c in context], axis=1)    \n",
    "    return concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_d, K_e, V_e = init_QKV()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_context = cross_context(context4, context_d_2, Q_d, K_e, V_e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add and Norm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_d_2 = np.random.rand(d_model) \n",
    "beta_d_2 = np.random.rand(d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_d_3 = add_norm(context_d_2, cross_context, gamma_d_2, beta_d_2) #residual for the next addNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feed Forward and Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2 = np.random.rand(d_model, d_model) \n",
    "b2 = np.random.rand(d_model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_d_4 = feed_forward(context_d_3, w2, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_d_3 = np.random.rand(d_model) \n",
    "beta_d_3 = np.random.rand(d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_d_5 = add_norm(context_d_4, context_d_3, gamma_d_3, beta_d_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LINEAR STEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_linear = np.random.rand(d_model, 3000000)\n",
    "b_linear = np.random.rand(3000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_prediction = context_d_5 @ w_linear + b_linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = tf.nn.softmax(raw_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Danware'"
      ]
     },
     "execution_count": 456,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "wv.index_to_key[np.argmax(prediction)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
