{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = api.load('glove-wiki-gigaword-50')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SETUP PARAMETER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 50\n",
    "n_head = 5\n",
    "n_batch = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(x):\n",
    "    return x.split()\n",
    "\n",
    "def embed(x):\n",
    "    embeddings = wv[x]\n",
    "    return embeddings\n",
    "\n",
    "def posEncode(x):\n",
    "    n_seq, _ = x.shape\n",
    "    i = np.arange(0, d_model, 2, dtype='float16')\n",
    "    denominator = np.power(10000, i/d_model)\n",
    "    position = np.arange(0, n_seq, dtype='float16').reshape(-1,1)\n",
    "    even_PE = np.sin(position / denominator)\n",
    "    odd_PE  = np.cos(position / denominator)\n",
    "    return x + np.ravel([even_PE.T, odd_PE.T],'F').reshape(n_seq, d_model)\n",
    "\n",
    "def masking(x):\n",
    "    mask = np.tril(np.ones((x.shape)))\n",
    "    mask[mask==0] = -np.infty\n",
    "    mask[mask==1] = 0\n",
    "    return x + mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_QKV(n_head, d_model):\n",
    "    Q = tf.Variable(np.random.rand(1,n_head, d_model, d_model//n_head), dtype='float32')\n",
    "    K = tf.Variable(np.random.rand(1,n_head, d_model, d_model//n_head), dtype='float32')\n",
    "    V = tf.Variable(np.random.rand(1,n_head, d_model, d_model//n_head), dtype='float32')\n",
    "    return Q, K, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context(input, Q,K,V, mask=False):    \n",
    "    raw_attention = ( input @ Q ) @ tf.transpose( input @ K, perm=(0,1,3,2) )\n",
    "    if (mask == True):\n",
    "        raw_attention = masking( raw_attention )\n",
    "    score = tf.nn.softmax( raw_attention / (d_model) ** .5 )\n",
    "    context = score @ ( input @ V )\n",
    "    \n",
    "    return context\n",
    "\n",
    "def concat4D(x):\n",
    "    a,b,c,d = x.shape \n",
    "    x1 = tf.transpose(x, perm=(0,1,3,2))\n",
    "    x2 = tf.reshape(x1, [a, 1, b*d, c])\n",
    "    x3 = tf.transpose(x2, perm=(0,1,3,2))\n",
    "    return x3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try AUTODIFF P1 -> Passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1, K1, V1 = init_QKV(n_head=n_head, d_model=d_model)\n",
    "x = np.random.rand(n_batch,1,5,d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "conxt = context(x, Q1,K1,V1)\n",
    "concat_conxt = concat4D(conxt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    tape.watch([Q1,K1,V1])\n",
    "    conxt = context(x,Q1,K1,V1)\n",
    "    concat = concat4D(conxt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_g_b(n_batch, d_model):\n",
    "    gamma = tf.Variable(np.ones((n_batch, 1, 1,d_model)), dtype='float32')\n",
    "    beta = tf.Variable(np.zeros((n_batch, 1, 1,d_model)), dtype='float32')\n",
    "    return gamma, beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_norm(context, prev_input, gamma, beta):\n",
    "    \n",
    "    context = context + prev_input\n",
    "    a,_,b,_ = context.shape\n",
    "    #________Mean__________\n",
    "    m = tf.reduce_mean(context, axis=3)\n",
    "    mean = tf.transpose(tf.reshape(m, [a,1,1,b]), perm=(0,1,3,2))\n",
    "    #________Sigma__________\n",
    "    s = tf.math.reduce_std(context, axis=3)\n",
    "    sigma = tf.transpose(tf.reshape(s, [a,1,1,b]), perm=(0,1,3,2))\n",
    "\n",
    "    context = (context - mean) / sigma\n",
    "    context = context * gamma + beta\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma1, beta1 = init_g_b(n_batch, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "input3 = add_norm(concat_conxt, x, gamma1, beta1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    tape.watch([Q1,K1,V1,gamma1,beta1])\n",
    "    conxt = context(x,Q1,K1,V1)\n",
    "    concat = concat4D(conxt)\n",
    "    AN = add_norm(concat, x, gamma1, beta1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add and Norm 1: \n",
    "Add&Norm: passed + \n",
    "gradient:passed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
