{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import gensim.downloader as api\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 25\n",
    "n_head = 5\n",
    "total_tokens = 1193514\n",
    "n_seq = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = api.load('glove-twitter-25')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "input1 = np.loadtxt('/Users/tanhoangminhco/Documents/Coding/Python/Machine Learning/Deep Learning/DLmodels/transformer/data/input1.csv')\n",
    "input2 = np.loadtxt('/Users/tanhoangminhco/Documents/Coding/Python/Machine Learning/Deep Learning/DLmodels/transformer/data/input2.csv')\n",
    "output = np.loadtxt('/Users/tanhoangminhco/Documents/Coding/Python/Machine Learning/Deep Learning/DLmodels/transformer/data/output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "input1 = input1.reshape(-1, 1, n_seq, 25)\n",
    "input2 = input2.reshape(-1, 1, n_seq, 25)\n",
    "OHoutput = tf.keras.utils.to_categorical(output, num_classes=total_tokens).reshape(-1,1,total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_batch, _, _, _ = input1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(x):\n",
    "    return x.split()\n",
    "\n",
    "def embed(x):\n",
    "    embeddings = wv[x]\n",
    "    return embeddings\n",
    "\n",
    "# def posEncode(x):\n",
    "#     _,_,n_seq, d_model = x.shape\n",
    "#     odd_i = np.arange(0, d_model, 2, dtype='float64')\n",
    "#     even_i = np.arange(1, d_model, 2, dtype='float64')\n",
    "#     odd_denominator = np.power(10000, odd_i/d_model)\n",
    "#     even_denominator = np.power(10000, even_i/d_model)\n",
    "#     position = np.arange(0, n_seq, dtype='float64').reshape(-1,1)\n",
    "#     even_PE = np.sin(position / even_denominator)\n",
    "#     odd_PE  = np.cos(position / odd_denominator)\n",
    "#     return x + np.ravel([even_PE.T, odd_PE.T],'F').reshape(n_seq, d_model)\n",
    "\n",
    "def masking(x):\n",
    "    mask = np.tril(np.ones((x.shape)))\n",
    "    mask[mask==0] = -np.infty\n",
    "    mask[mask==1] = 0\n",
    "    return x + mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_QKV(n_head, d_model):\n",
    "    Q = tf.Variable(np.random.rand(1,n_head, d_model, d_model//n_head), dtype='float64')\n",
    "    K = tf.Variable(np.random.rand(1,n_head, d_model, d_model//n_head), dtype='float64')\n",
    "    V = tf.Variable(np.random.rand(1,n_head, d_model, d_model//n_head), dtype='float64')\n",
    "    return Q, K, V\n",
    "\n",
    "def init_GB(n_batch, d_model):\n",
    "    gamma = tf.Variable(np.ones((n_batch, 1, 1,d_model)), dtype='float64')\n",
    "    beta = tf.Variable(np.zeros((n_batch, 1, 1,d_model)), dtype='float64')\n",
    "    return gamma, beta\n",
    "\n",
    "def init_WB(input_size, neurons):\n",
    "    w = tf.Variable(np.random.rand(1,1, input_size, neurons), dtype= 'float64')\n",
    "    b = tf.Variable(np.zeros((n_batch, 1, 1, neurons)) , dtype= 'float64')\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context(input, Q,K,V, cross_input=None, mask=False):    \n",
    "    if (cross_input==None):\n",
    "        cross_input = input\n",
    "    raw_attention = ( cross_input @ Q ) @ tf.transpose( input @ K, perm=(0,1,3,2) )\n",
    "    if (mask == True):\n",
    "        raw_attention = masking( raw_attention )\n",
    "    score = tf.nn.softmax( raw_attention / (d_model) ** .5 )\n",
    "    context = score @ ( input @ V )\n",
    "    \n",
    "    return context\n",
    "\n",
    "def concat4D(x):\n",
    "    a,b,c,d = x.shape \n",
    "    x1 = tf.transpose(x, perm=(0,1,3,2))\n",
    "    x2 = tf.reshape(x1, [a, 1, b*d, c])\n",
    "    x3 = tf.transpose(x2, perm=(0,1,3,2))\n",
    "    return x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_norm(context, prev_input, gamma, beta):\n",
    "    \n",
    "    context = context + prev_input\n",
    "    a,_,b,_ = context.shape\n",
    "    #________Mean__________\n",
    "    m = tf.reduce_mean(context, axis=3)\n",
    "    mean = tf.transpose(tf.reshape(m, [a,1,1,b]), perm=(0,1,3,2))\n",
    "    #________Sigma__________\n",
    "    s = tf.math.reduce_std(context, axis=3)\n",
    "    sigma = tf.transpose(tf.reshape(s, [a,1,1,b]), perm=(0,1,3,2))\n",
    "\n",
    "    context = (context - mean) / sigma\n",
    "    context = context * gamma + beta\n",
    "    \n",
    "    return context\n",
    "\n",
    "def feed_forward(x, w, b):\n",
    "    z = x @ w + b\n",
    "    a = tf.nn.relu(z)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAINING || Assume no positional encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1, K1, V1 = init_QKV(n_head=n_head, d_model=d_model)\n",
    "gamma1, beta1 = init_GB(n_batch=n_batch, d_model=d_model)\n",
    "gamma2, beta2 = init_GB(n_batch=n_batch, d_model=d_model)\n",
    "w1, b1 = init_WB(d_model, 16)\n",
    "w2, b2 = init_WB(16, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "context1 = concat4D(context(input1,Q1, K1, V1))\n",
    "input1_1 = add_norm(context1, input1, gamma1, beta1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "A1 = feed_forward(input1_1, w1, b1)\n",
    "A2 = feed_forward(A1, w2, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "input1_2 = add_norm(A2, input1_1, gamma2, beta2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DECODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2, K2, V2 = init_QKV(n_head=n_head, d_model=d_model)\n",
    "Q_d, K_e, V_e = init_QKV(n_head=n_head, d_model=d_model)\n",
    "gamma3, beta3 = init_GB(n_batch=n_batch, d_model=d_model)\n",
    "gamma4, beta4 = init_GB(n_batch=n_batch, d_model=d_model)\n",
    "gamma5, beta5 = init_GB(n_batch=n_batch, d_model=d_model)\n",
    "w3, b3 = init_WB(d_model, 16)\n",
    "w4, b4 = init_WB(16, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "context2 = concat4D(context(input2,Q2, K2, V2, mask=True))\n",
    "input2_1 = add_norm(context2, input2, gamma3, beta3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_context = concat4D(context(input1_2,Q2, K2, V2, cross_input=input2_1))\n",
    "input2_2 = add_norm(cross_context, input2_1, gamma4, beta4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "A3 = feed_forward(input2_2, w3, b3)\n",
    "A4 = feed_forward(A3, w4, b4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "input2_3 = add_norm(A4, input2_2, gamma5, beta5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(np.random.rand(1, 25, total_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = tf.nn.softmax(input2_3[:, :, -1, :] @ W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars =          [Q1,K1,V1,\n",
    "                 gamma1, beta1, gamma2, beta2,\n",
    "                 w1,b1,w2,b2,\n",
    "                 Q2, K2, V2, \n",
    "                 Q_d, K_e, V_e,\n",
    "                 gamma3,beta3,gamma4,beta4,gamma5,beta5,\n",
    "                 w3,b3,w4,b4, W]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        tape.watch(vars)\n",
    "\n",
    "        #ENCODER\n",
    "        context1 = concat4D(context(input1,Q1, K1, V1))\n",
    "        input1_1 = add_norm(context1, input1, gamma1, beta1)\n",
    "        A1 = feed_forward(input1_1, w1, b1)\n",
    "        A2 = feed_forward(A1, w2, b2)\n",
    "        input1_2 = add_norm(A2, input1_1, gamma2, beta2)\n",
    "        #DECODER\n",
    "        context2 = concat4D(context(input2,Q2, K2, V2, mask=True))\n",
    "        input2_1 = add_norm(context2, input2, gamma3, beta3)\n",
    "        cross_context = concat4D(context(input1_2,Q2, K2, V2, cross_input=input2_1))\n",
    "        input2_2 = add_norm(cross_context, input2_1, gamma4, beta4)\n",
    "        A3 = feed_forward(input2_2, w3, b3)\n",
    "        A4 = feed_forward(A3, w4, b4)\n",
    "        input2_3 = add_norm(A4, input2_2, gamma5, beta5)\n",
    "        predictions = tf.nn.softmax(input2_3[:, :, -1, :] @ W)\n",
    "        loss = tf.reduce_mean(tf.square(predictions - OHoutput))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
