{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import gensim.downloader as api\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = api.load('glove-twitter-25')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/Users/tanhoangminhco/Downloads/Downloads/reddit_worldnews_start_to_2016-11-22.csv')\n",
    "data = data['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = data[:10]\n",
    "max_in = 16\n",
    "n_dim = 25\n",
    "total_tokens = 1193514"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count(tweets):\n",
    "    max = 0\n",
    "    for tweet in tweets:\n",
    "        if len(tweet.split()) > max:\n",
    "            max =  len(tweet.split())\n",
    "    return max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(tokens, max_input):\n",
    "    return embedRecur(tokens, index=0, max_input=max_input, array=[])\n",
    "\n",
    "def embedRecur(tokens, index, max_input, array):\n",
    "    if index==len(tokens):\n",
    "        if index < max_input:\n",
    "            for i in range(index, max_input):\n",
    "                array.append(np.zeros(n_dim))\n",
    "        return np.array(array)\n",
    "    try:\n",
    "        emb = wv[tokens[index]]\n",
    "    except KeyError:\n",
    "        array.append(wv['unk'])\n",
    "    else:\n",
    "        array.append(emb)\n",
    "\n",
    "    return embedRecur(tokens, index + 1, max_input, array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "input1 = []\n",
    "input2 = []\n",
    "output = []\n",
    "for tweet in tweets:\n",
    "    tokens = tweet.lower().split()\n",
    "    for i in range(1, len(tokens)-1):\n",
    "        in1_tokens = tokens[:i]\n",
    "        in1 = embed(in1_tokens, max_in)\n",
    "        in1 = in1.reshape(1,max_in,n_dim)\n",
    "\n",
    "        for y in range(i, len(tokens)):\n",
    "            input1.append(in1)\n",
    "            in2_tokens = ['start'] + tokens[i: y]\n",
    "            in2 = embed(in2_tokens, max_in)\n",
    "            input2.append(in2.reshape(1,max_in,n_dim))\n",
    "            try:\n",
    "                index = wv.key_to_index[tokens[y]]\n",
    "            except KeyError:\n",
    "                index = wv.key_to_index['unk']\n",
    "            \n",
    "            # out = tf.keras.utils.to_categorical(index, num_classes=total_tokens, dtype='float16').reshape(1,1,-1)\n",
    "            output.append(index)\n",
    "\n",
    "\n",
    "input1 = np.array(input1)\n",
    "input2 = np.array(input2)\n",
    "output = np.array(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "input1_to_file = input1.reshape(-1, n_dim)\n",
    "np.savetxt('/Users/tanhoangminhco/Documents/Coding/Python/Machine Learning/Deep Learning/DLmodels/transformer/data/input1.csv',\n",
    "           input1_to_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "input2_to_file = input2.reshape(-1, n_dim)\n",
    "np.savetxt('/Users/tanhoangminhco/Documents/Coding/Python/Machine Learning/Deep Learning/DLmodels/transformer/data/input2.csv',\n",
    "           input2_to_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_to_file = output\n",
    "\n",
    "np.savetxt('/Users/tanhoangminhco/Documents/Coding/Python/Machine Learning/Deep Learning/DLmodels/transformer/data/output.csv',\n",
    "           output_to_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
